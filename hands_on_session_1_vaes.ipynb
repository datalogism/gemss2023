{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ_pmgxvGur9"
   },
   "source": [
    "# Hands-on session 1- Variational Auto-Encoders\n",
    "## Generative Modeling Summer School 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEneMITS2agU"
   },
   "source": [
    "#### Instructions on how to use this notebook:\n",
    "\n",
    "This notebook is hosted on ``Google Colab``. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
    "\n",
    "You can also avoid using ``Colab`` entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
    "\n",
    "The advantage of using **Colab** is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for various models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
    "\n",
    "The default ``Colab`` runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
    "\n",
    "```sh\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "Note that despite the name, ``Google Colab`` does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
    "\n",
    "**Submission:** Please bring your (partial) solution to the hands-on session. Then you can discuss it with intructors and your colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBgoJIpdLI2Y",
    "outputId": "4c660034-9145-4684-91b4-c37cf18e1e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsdc7fDp40rQ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, we are going to implement a Variational Auto-Encoder (VAE). A VAE is a likelihood-based deep generative model that consists of a stochastic encoder (a variational posterior over latent variables), a stochastic decoder, and a marginal distribution over latent variables (a.k.a. a prior). The model was originally proposed in two concurrent papers:\n",
    "- [Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.](https://arxiv.org/abs/1312.6114)\n",
    "- [Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. \"Stochastic backpropagation and approximate inference in deep generative models.\" International conference on machine learning. PMLR, 2014.](https://proceedings.mlr.press/v32/rezende14.html)\n",
    "\n",
    "You can read more about VAEs in Chapter 4 of the following book:\n",
    "- [Tomczak, J.M., \"Deep Generative Modeling\", Springer, 2022](https://link.springer.com/book/10.1007/978-3-030-93158-2)\n",
    "\n",
    "In particular, the goals of this assignment are the following:\n",
    "\n",
    "- Understand how VAEs are formulated\n",
    "- Implement components of VAEs using PyTorch\n",
    "- Train and evaluate a model for image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvsuVNczG6pP"
   },
   "source": [
    "### Theory behind VAEs\n",
    "\n",
    "VAEs are latent variable models trained with variational inference. In general, the latent variable models define the following generative process:\n",
    "\\begin{align}\n",
    "1.\\ & \\mathbf{z} \\sim p_{\\lambda}(\\mathbf{z}) \\\\\n",
    "2.\\ & \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x}|\\mathbf{z})\n",
    "\\end{align}\n",
    "\n",
    "In plain words, we assume that for observable data $\\mathbf{x}$, there are some latent (hidden) factors $\\mathbf{z}$. Then, the training objective is the log-likelihood function of the following form:\n",
    "$$\n",
    "\\log p_{\\vartheta}(\\mathbf{x})=\\log \\int p_\\theta(\\mathbf{x} \\mid \\mathbf{z}) p_\\lambda(\\mathbf{z}) \\mathrm{d} \\mathbf{z} .\n",
    "$$\n",
    "\n",
    "The problem here is the intractability of the integral if the dependencies between random variables $\\mathbf{x}$ and $\\mathbf{z}$ are non-linear and/or the distributions are non-Gaussian.\n",
    "\n",
    "By introducing variational posteriors $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$, we get the following lower bound (the Evidence Lower Bound, ELBO):\n",
    "$$\n",
    "\\log p_{\\vartheta}(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_\\theta(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\mathrm{KL}\\left(q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p_\\lambda(\\mathbf{z})\\right) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suzhlbWqxtD9"
   },
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjxkigYLxpB7",
    "outputId": "79f47a34-017a-4767-ae39-d15bf856a17f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_model_summary in /root/anaconda3/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/lib/python3.8/site-packages (from pytorch_model_summary) (4.65.0)\n",
      "Requirement already satisfied: numpy in /root/anaconda3/lib/python3.8/site-packages (from pytorch_model_summary) (1.24.2)\n",
      "Requirement already satisfied: torch in /root/anaconda3/lib/python3.8/site-packages (from pytorch_model_summary) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /root/anaconda3/lib/python3.8/site-packages (from torch->pytorch_model_summary) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# DO NOT REMOVE!\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "!pip install pytorch_model_summary\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm23hRm6CqGh",
    "outputId": "bb96c549-e9d0-42ff-eef8-2466382798a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available device is cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and determine the device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "\n",
    "device = 'cpu'\n",
    "print(f'The available device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81CxONpmMulC",
    "outputId": "29762cfe-f956-4e78-8f8e-1691ea373510"
   },
   "outputs": [],
   "source": [
    "# mount drive: WE NEED IT FOR SAVING IMAGES!\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "OoPb92zNM4UY"
   },
   "outputs": [],
   "source": [
    "# PLEASE CHANGE IT TO YOUR OWN GOOGLE DRIVE!\n",
    "images_dir = '/user/cringwal/home/Desktop/THESE_YEAR1/GEMSS2023/Assignements/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3zs31tOyCmq"
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF0agzL7tDHK"
   },
   "source": [
    "Let us define some useful log-distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "LIBNVRNJtHSd"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE\n",
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2LLOs0kn7iw"
   },
   "source": [
    "## Implementing VAEs\n",
    "\n",
    "The goal of this assignment is to implement four classes:\n",
    "- `Encoder`: this class implements the encoder (variational posterior), $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$.\n",
    "- `Decoder`: this class implements the decoded (the conditional likelihood), $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$.\n",
    "- `Prior`: this class implements the marginal over latents (the prior), $p_{\\lambda}(\\mathbf{z})$.\n",
    "- `VAE`: this class combines all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cXhOwKAzW6Z"
   },
   "source": [
    "### Encoder\n",
    "We start with `Encoder`. Please remember that we assume the Gaussian variational posterior with a diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "MrwQXSuEoFfH"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTE: The class must containt the following functions:\n",
    "# (i) reparameterization\n",
    "# (ii) sample\n",
    "# (iii) log_prob\n",
    "# Moreover, forward must return the log-probability of the variational posterior for given x, i.e., log q(z|x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):  # ADD APPROPRIATE ATTRIBUTES\n",
    "        super(Encoder, self).__init__()\n",
    "        # Init of encoder network\n",
    "        self.encoder = encoder_net\n",
    "        \n",
    "    @staticmethod\n",
    "    # Reparametrization trick for Gaussians\n",
    "    def reparameterization(mu, log_var):\n",
    "        # std for log-variance\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        #sample of epsilon from normal(0,1)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu+std*eps\n",
    "\n",
    "    # Output of encoder network (parameter of gausssian)\n",
    "    def encode(self, x):\n",
    "        # encoder network of size 2M\n",
    "        #print(\"ENCODE :)\")\n",
    "        h_e =  self.encoder(x)\n",
    "        #print(\"now get mu and logvare\")\n",
    "        # divide layer into mean / std vectors\n",
    "        mu_e, log_var_e= torch.chunk(h_e, 2,dim=1)\n",
    "\n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling process\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if (mu_e is None) and (log_var_e is None):\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None):\n",
    "                raise ValueError('mu and log-scale can`t be None!')\n",
    "            z = self.reparameterization(mu_e, log_var_e)\n",
    "        return z\n",
    "\n",
    "    # Log-proba used for ELBO\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
    "                raise ValueError('mu, log-scale and z can`t be None!')\n",
    "\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XVlH5OUzdgJ"
   },
   "source": [
    "Please answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1BNAH02zjjt"
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "Please explain the reparameterization trick and provide a mathematical formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxYq7-gzo_o"
   },
   "source": [
    "**ANSWER**:\n",
    "We use a sample operation from the variationnal posterior (MC approximation) for estimate ELBO. This operation make impossible the backprogation of the gradient.\n",
    "For overpassing that problem we define a new independant and random variable ε ~ N(0,1). Then we use by decomposing the latent variable z, by considering it as a Gaussian variable, depending of  ε, a mean (μ) and a variance (σ²) as follow :\n",
    "$$\n",
    "\\mathbf{z}=\\mathbf{μ}+\\mathbf{σ}.ε\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpRgXdtBzt3-"
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "Please write down mathematically the log-probability of the encoder (variational posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET-mMAg10Ewv"
   },
   "source": [
    "**REMINDERS**\n",
    "* As p(X) is intractable and the approximation by the Monte Carlo sampling expensive we prefer to modelise it via a **variationnal inference** :\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{x}) & = \\ln \\int p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} \\\\\n",
    "& = \\ln \\int \\frac{q_{\\phi}(\\mathbf{z})}{q_{\\phi}(\\mathbf{z})} p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} \\\\\n",
    "& = \\ln \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z})} \\left[ \\frac{p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})}{q_{\\phi}(\\mathbf{z}) } \\right] \\\\\n",
    "&\\geq \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z})} \\ln \\left[ \\frac{p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})}{q_{\\phi}(\\mathbf{z}) } \\right] \\\\\n",
    "&\\geq \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\mathbf{z}) + \\ln p(\\mathbf{z}) - \\ln q_{\\phi}(\\mathbf{z}) \\right] \\\\\n",
    "&\\geq\\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\mathbf{z}) \\right] - \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z})} \\left[ \\ln q_{\\phi}(\\mathbf{z}) -  \\ln p(\\mathbf{z}) \\right]\n",
    "\\end{align}\n",
    "\n",
    "*  The **amortized variational posterior** process allows us to learn a function $q_{\\phi}(z|x)$ via a neural network on given inputs to returns an approximation of a chosen input distribution parameters:\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z|x})} \\left[ \\ln p(\\mathbf{x} | \\mathbf{z}) \\right] - \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z|x})} \\left[ \\ln q_{\\phi}(\\mathbf{z|x}) -  \\ln p(\\mathbf{z}) \\right]\n",
    "\\end{align}\n",
    "* This lower-bound is the famous **ELBO**, we already told about. The first part is the **reconstruction error** that we want to **maximize**, and the second is the **Kullback-Leibler divergence** (a kind of regulizer) that must be **minimized** :\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z|x})} \\left[ \\ln p(\\mathbf{x} | \\mathbf{z}) \\right] - \\mathbb{KL}[q_{\\phi}({x|z}) || p(z)]\n",
    "\\end{align}\n",
    "* Here we want optimize the last equation by finding the best $q_{\\phi}({x|z})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKaiNG5O3CWU"
   },
   "source": [
    "**ANSWER :**\n",
    "\n",
    "The distribution of variationnal posterior is defined by $qϕ(x|z)$ as following a Gaussian distrib.\n",
    "\n",
    "$qϕ(x|z) \\sim N(z|\\mu_ϕ(x),diag[σ²_ϕ(x)])$\n",
    "\n",
    "where $ϕ(x), σ²_ϕ$ are two parameters learnt by a NN\n",
    "\n",
    "* the log-proba computed as :\n",
    "\n",
    "$ \\ln qϕ(x|z) = -\\frac{n}{2}\\ln(2π) - \\frac{n}{2}\\ln(σ²)-\\frac{1}{2σ²}\\sum(x_i-\\mu)²$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhNTy5mn0XDT"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder is the conditional likelihood, i.e., $p(x|z)$. Please remember that we must decide on the form of the distribution (e.g., Bernoulli, Gaussian, Categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "9vTmKHwrpUVa"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTE: The class must containt the following functions:\n",
    "# (i) sample\n",
    "# (ii) log_prob\n",
    "# Moreover, forward must return the log-probability of the conditional likelihood function for given z, i.e., log p(x|z)\n",
    "# Additionally, please specify the distribution class you want to use for the decode (i.e. the `distribution` attribute)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None): # ADD APPROPRIATE ATTRIBUTES\n",
    "        super(Decoder, self).__init__()\n",
    "        # Decoder init\n",
    "        self.decoder = decoder_net\n",
    "        # Distribution used for encoder\n",
    "        self.distribution = distribution\n",
    "        # nb of possibles values\n",
    "        self.num_vals=num_vals\n",
    "\n",
    "\n",
    "    # Get param of likelihood function p(x|y)\n",
    "    def decode(self, z):\n",
    "        # decoder network\n",
    "        h_d = self.decoder(z)\n",
    "        # Depends of distrib\n",
    "        if( self.distribution=='categorical'):\n",
    "            # batch size\n",
    "            b = h_d.shape[0]\n",
    "            # dim of x\n",
    "            d = h_d.shape[1]//self.num_vals\n",
    "            #reshape to batch size / dim . nb values\n",
    "            h_d = h_d.view(b, d, self.num_vals)\n",
    "            # get proba from softmax\n",
    "            mu_d = torch.softmax(h_d, 2)\n",
    "            return [mu_d]\n",
    "          \n",
    "        elif self.distribution == 'bernoulli':\n",
    "            # return a single proba in Bernouilli case\n",
    "            mu_d = torch.sigmoid(h_d)\n",
    "            return [mu_d]\n",
    "        else :\n",
    "            raise ValueError(\"Only : 'categorial' and 'bernouilli' implemented distrib\")\n",
    "\n",
    "    # SAMPLING FROM DECODER\n",
    "    def sample(self, z):\n",
    "        outs = self.decode(z)\n",
    "        if( self.distribution=='categorical'):\n",
    "          # output of decoder\n",
    "            mu_d = outs[0]\n",
    "          # batch size of decoder\n",
    "            b = mu_d.shape[0]\n",
    "          # dim of of decoder\n",
    "            m = mu_d.shape[1]\n",
    "          #reshape to batch size / dim . nb values\n",
    "            mu_d = mu_d.view(mu_d.shape[0], -1, self.num_vals)\n",
    "          # sample the categorials\n",
    "            p = mu_d.view(-1, self.num_vals)\n",
    "            \n",
    "            x_new = torch.multinomial(p, num_samples=1).view(b, m)\n",
    "        elif self.distribution == 'bernoulli':\n",
    "          # no reshaping\n",
    "          mu_d = outs[0]\n",
    "          # pytorch bernoulli sampler :)\n",
    "          x_new = torch.bernoulli(mu_d)\n",
    "        else :\n",
    "              raise ValueError(\"Only : 'categorial' and 'bernouilli' implemented distrib\")\n",
    "        return x_new\n",
    "\n",
    "    # Compute conditional log-likelihood function\n",
    "    def log_prob(self, x, z):\n",
    "        outs = self.decode(z)\n",
    "\n",
    "        if self.distribution == 'categorical':\n",
    "            mu_d = outs[0]\n",
    "            log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum', dim=-1).sum(-1)\n",
    "\n",
    "        elif self.distribution == 'bernoulli':\n",
    "            mu_d = outs[0]\n",
    "            log_p = log_bernoulli(x, mu_d, reduction='sum', dim=-1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Either `categorical` or `bernoulli`')\n",
    "\n",
    "        return log_p\n",
    "\n",
    "    def forward(self, z, x=None, type='log_prob'):\n",
    "        assert type in ['decoder', 'log_prob'], 'Type could be either decode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x, z)\n",
    "        else:\n",
    "            return self.sample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xjbNkNL01DP"
   },
   "source": [
    "Please answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjDvPaBj04cA"
   },
   "source": [
    "#### Question 3\n",
    "\n",
    "Please explain your choice of distribution for image data used in this assignment. Additionally, please write it down mathematically (if you think that presenting it as the log-probability, then please do it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLZEzmGI1Ok-"
   },
   "source": [
    "ANSWER:\n",
    "We are trying with the decoder to estimate the probability of a given image $x$ by knowing $z$, with have different option concerning the choise of the distribution of the images :\n",
    "\n",
    "* The Bernoulli distribution :\n",
    "$$p_θ(\\mathbf{x}|z) \\sim Bernouilli(x|θ(z)) $$ with\n",
    "\\begin{align}\n",
    "\\mathbf{x} \\in \\{0, 1\\}\\\\\n",
    "Θ \\in \\{0, 1\\} \\\\\n",
    "  Bernouilli(x,Θ) = Θ^x(1-Θ)^{1-x} \\\\\n",
    "  \\ln(Bernouilli(x,Θ)) = \\ln(Θ^x(1-Θ)^{1-x})\n",
    "\\end{align}\n",
    "\n",
    "* The  categorial distribution :\n",
    "$$p_θ(\\mathbf{x}|z) \\sim Categorical(x|θ(z)) $$ with\n",
    "\\begin{align}   \n",
    "  \\mathbf{x} \\in \\{0, 1\\} \\\\\n",
    "  \\mathbf{θ}^d \\in \\{0, 1\\} \\\\\n",
    "  d \\in [0, 255] \\\\\n",
    "  Categorical(x,Θ) = ∑^{255}_{d=0}Θ^{xd}_{d} \\\\\n",
    "  \\ln(Categorical(x,Θ)) = \\ln (∑^{255}_{d=0}Θ^{xd}_{d})\n",
    "\\end{align}\n",
    "\n",
    "> **As we are working with black and white MNIST images we are using the Bernoulli distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhbWamId1eGt"
   },
   "source": [
    "#### Question 4\n",
    "\n",
    "Please explain how one can sample from the distribution chosen by you. Please be specific and formal (i.e., provide mathematical formulae)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-CfWmtUuiU2"
   },
   "source": [
    "**ANSWER :**\n",
    "\n",
    "For sampling a given $p_θ(\\mathbf{x}|z)$ we need first to have on hand a latent vector $z$. ( from this one we will be able from the decoder to get $θ(z)$ that is a vector representing the probabilities to obtain a white pixel for each position of the image (in the case of ther Bernoulli distribution) or a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLIEwIiw00op"
   },
   "source": [
    "### Prior\n",
    "\n",
    "The prior is the marginal distribution over latent variables, i.e., $p(z)$. It plays a crucial role in the generative process and also in synthesizing images of better quality.\n",
    "\n",
    "In this assignment, you are asked to implement a prior that is learnable (e.g., parameterized by neural networks). If you decide to implement the standard Gaussian prior only, then please be aware that you will not get any points.\n",
    "\n",
    "For the learnable prior you can choose the **Mixture of Gaussians**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "EReSv_tWX7vH"
   },
   "outputs": [],
   "source": [
    "#### STANDARD GAUSSIAN\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, L=2, num_components=1):\n",
    "        super(Prior, self).__init__()\n",
    "\n",
    "        self.L = L\n",
    "\n",
    "        # params weights\n",
    "        self.means = torch.zeros(1, L)\n",
    "        self.logvars = torch.zeros(1, L)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.means, self.logvars\n",
    "#\n",
    "    def sample(self, batch_size):\n",
    "        return torch.randn(batch_size, self.L)\n",
    "#\n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "xQIvee5Cp69V"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTES:\n",
    "# (i) The function \"sample\" must be implemented.\n",
    "# (ii) The function \"forward\" must return the log-probability, i.e., log p(z)\n",
    "\n",
    "################ MIXTURE OF GAUSSIANS\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, L, num_components=2):\n",
    "        super(Prior, self).__init__()\n",
    "\n",
    "        self.L = L\n",
    "        self.num_components = num_components\n",
    "\n",
    "      # params\n",
    "      # What is multiplier var ?\n",
    "        multiplier=1\n",
    "        self.means = nn.Parameter(torch.randn(num_components, self.L)*multiplier)\n",
    "        self.logvars = nn.Parameter(torch.randn(num_components, self.L))\n",
    "\n",
    "      # mixing weights\n",
    "        self.w = nn.Parameter(torch.zeros(num_components, 1, 1))\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.means, self.logvars\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # mu, lof_var\n",
    "        means, logvars = self.get_params()\n",
    "\n",
    "        # mixing probabilities\n",
    "        w = F.softmax(self.w, dim=0)\n",
    "        w = w.squeeze()\n",
    "\n",
    "        # pick components\n",
    "        indexes = torch.multinomial(w, batch_size, replacement=True)\n",
    "\n",
    "        # means and logvars\n",
    "        eps = torch.randn(batch_size, self.L)\n",
    "        for i in range(batch_size):\n",
    "            indx = indexes[i]\n",
    "            if i == 0:\n",
    "                z = means[[indx]] + eps[[i]] * torch.exp(logvars[[indx]])\n",
    "            else:\n",
    "                z = torch.cat((z, means[[indx]] + eps[[i]] * torch.exp(logvars[[indx]])), 0)\n",
    "        return z\n",
    "\n",
    "\n",
    "    # Log-proba used for ELBO\n",
    "    def log_prob(self, z):\n",
    "        \n",
    "        #print(\"HEY ICI\")\n",
    "        # mu, lof_var\n",
    "        means, logvars = self.get_params()\n",
    "\n",
    "        # mixing probabilities\n",
    "        w = F.softmax(self.w, dim=0)\n",
    "\n",
    "        # log-mixture-of-Gaussians\n",
    "        z = z.unsqueeze(0) # 1 x B x L\n",
    "        means = means.unsqueeze(1) # K x 1 x L\n",
    "        logvars = logvars.unsqueeze(1) # K x 1 x L\n",
    "\n",
    "        log_p = log_normal_diag(z, means, logvars) + torch.log(w) # K x B x L\n",
    "        log_prob = torch.logsumexp(log_p, dim=0, keepdim=False) # B x L\n",
    "        #print(\"supposed size\")\n",
    "        #print(log_prob.shape)\n",
    "        return log_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0KH9f2O_PDg"
   },
   "source": [
    "#### Question 5 > ADD SAMPLING PROCESS AND LOG PROBA\n",
    "\n",
    "**Option 1:  Standard Gaussian**\n",
    "\n",
    "- Please explain the choice of your prior and write it down mathematically.\n",
    "\n",
    "**Option 2: Mixture of Gaussians**\n",
    "\n",
    "Please do the following:\n",
    "- Please explain the choice of your prior and write it down mathematically.\n",
    "- Please write down its sampling procedure (if necessary, please add a code snippet).\n",
    "- Please write down its log-probability (a mathematical formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNl9LkeewTD5"
   },
   "source": [
    "**ANSWER OPTION 1 :**\n",
    "\n",
    "If i had to chose this option i would design it as follow :\n",
    "$$p(z) \\sim \\mathcal{N}(z|0,I) $$\n",
    "But suing a fixed distribution could led to :  \n",
    "* posterior collapse : the regularisation term is minimized when $∀_xq_ϕ(z|x)=p(z)$ and sometimes the decoder could treets z as a noise.\n",
    "* the hole problem : when a mismatch append between the aggregated posterior (could be for example the mean of var posteriors over all training data) and the prior. For example a high probability prior with a low postior proba and vise-versa. Sampling in theses kind of hole will produce unrealistic latent and impact the qualityof the outputs.\n",
    "\n",
    "A more general problem, also more globally related to deep generative  models : the out-of-distribution. Because VAE poorly detect out of distrib examples that generally relate to other distribution.\n",
    "\n",
    "\n",
    "**ANSWER OPTION 2 :**\n",
    "A second option is to design it via a mixture of $\\mathbf{K}$ Gaussians, patching the holes and fitting better to the aggregated posterior :\n",
    "\n",
    "$$ p_λ(z) = ∑^K_{k=1}W_k\\mathcal{N}(z|μ_k,\\sigma²_k)$$\n",
    "with the following trainable parameters : $$λ = \\{\\{w_k\\},\\{μ_k\\},\\{σ²_k\\}\\} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOM4QM9I_62d"
   },
   "source": [
    "### Complete VAE\n",
    "\n",
    "The last class is `VAE` tha combines all components. Please remember that this class must implement the **Negative ELBO** in `forward`, as well as `sample` (*hint*: it is a composition of `sample` functions from the prior and the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "OQpf-BeSqA6V"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# This class combines Encoder, Decoder and Prior.\n",
    "# NOTES:\n",
    "# (i) The function \"sample\" must be implemented.\n",
    "# (ii) The function \"forward\" must return the negative ELBO. Please remember to add an argument \"reduction\", which is either \"mean\" or \"sum\".\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, prior, num_vals=256, L=16, likelihood_type='categorical'): # prior > nb of Gaussians\n",
    "        # num_compo > nb dimension of latent space\n",
    "        # num_vals > nb of pixel categories\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by CR.')\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "\n",
    "    def forward(self, x, reduction='mean'):\n",
    "        # output: the negative ELBO (NELBO) that is either averaged or summed (VERY IMPORTANT!)\n",
    "\n",
    "        #print(\"in foward\")\n",
    "\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "\n",
    "        #print(\"mu, var : \",mu_e,\"-\",log_var_e)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # ELBO\n",
    "        RE = self.decoder.log_prob(x, z) # RECONSTRUCTION ERROR\n",
    "        logprobz=self.prior.log_prob(z)\n",
    "        #print(\"logprobz\")\n",
    "        #print(logprobz.shape)\n",
    "        encologprob=self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)\n",
    "        #print(\"encologprob\")\n",
    "        #print(encologprob.shape)\n",
    "        \n",
    "        KL = (logprobz - encologprob).sum(-1)# REGULIZER\n",
    "\n",
    "        error = 0\n",
    "        if np.isnan(RE.detach().numpy()).any():\n",
    "            print('RE {}'.format(RE))\n",
    "            error = 1\n",
    "        if np.isnan(KL.detach().numpy()).any():\n",
    "            print('RE {}'.format(KL))\n",
    "            error = 1\n",
    "\n",
    "        if error == 1:\n",
    "            raise ValueError()\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return -(RE + KL).sum()\n",
    "        else:\n",
    "            return -(RE + KL).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9axMlEkAYN5"
   },
   "source": [
    "#### Question 6 > TO COMPLETE NEED MORE EXPLANATIONS\n",
    "\n",
    "Please explain your choice of the distribution for the conditional likelihood function, and write down mathematically the log-probability of the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgbEJm8FAuze"
   },
   "source": [
    "ANSWER: I'm not sure to understand the question but ...\n",
    "\n",
    "The conditionnal likelihood is $p(z|x)$ also noted as $p_θ(x)$ :\n",
    "$$ p_θ(\\mathbf{x}) = \\int p_θ(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} $$\n",
    "this probability is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaJgXPYyAmeJ"
   },
   "source": [
    "#### Question 7\n",
    "\n",
    "Please write down mathematically the **Negative ELBO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THYyO-G7AkSQ"
   },
   "source": [
    "**ANSWER:**\n",
    "\n",
    "As a lot of package we generally minimize a training objective, we have to use the negative ELBO\n",
    "$$ −ELBO(D; θ, ϕ) = \\sum^N_{n=1}-\\{\\ \\ln Categorial(x_n|θ(z_{ϕ,n})) + [\\ln N(z_{ϕ,n}|μ_ϕ(X_n),σ²_ϕ(x_n)+\\ln N(z_{ϕ,n|0,I}]\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLhgze7DA4yx"
   },
   "source": [
    "### Evaluation and training functions\n",
    "\n",
    "**Please DO NOT remove or modify them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "I9Dr3a6lqJ0W"
   },
   "outputs": [],
   "source": [
    "# ==========DO NOT REMOVE==========\n",
    "\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, (test_batch, _) in enumerate(test_loader):\n",
    "        test_batch = test_batch.to(device)\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader, shape=(28,28)):\n",
    "    # real images-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], shape)\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, shape=(28,28), extra_name=''):\n",
    "    x, _ = next(iter(data_loader))\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    # generations-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], shape)\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "9ABgMeG0qFAP"
   },
   "outputs": [],
   "source": [
    "# ==========DO NOT REMOVE==========\n",
    "\n",
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, shape=(28,28)):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        print(\">>>>>>>>>>>>> epoch \",e,\"/\",num_epochs)\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        #print(\"aftertrain >ep \",num_epochs)\n",
    "        for indx_batch, (batch, _) in enumerate(training_loader):\n",
    "            model = model.to(device)\n",
    "            batch = batch.to(device)\n",
    "            #print(\"batch/\")\n",
    "            loss = model.forward(batch, reduction='mean')\n",
    "            #print(\"/loss\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print(\"end loop\")\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "                \n",
    "                samples_generated(name, val_loader, shape=shape, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWr8N2u2qNTu"
   },
   "source": [
    "### Setup\n",
    "\n",
    "**NOTE: *Please comment your code! Especially if you introduce any new variables (e.g., hyperparameters).***\n",
    "\n",
    "In the following cells, we define `transforms` for the dataset. Next, we initialize the data, a directory for results and some fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "bFTE5jtYpxDV"
   },
   "outputs": [],
   "source": [
    "# PLEASE DEFINE APPROPRIATE TRANFORMS FOR THE DATASET\n",
    "# (If you don't see any need to do that, then you can skip this cell)\n",
    "# HINT: Please prepare your data accordingly to your chosen distribution in the decoder\n",
    "from torchvision import transforms\n",
    "transforms_train = torchvision.transforms.Compose( [\n",
    "                                                    #transforms.Resize((28,28)),\n",
    "                                                    transforms.ToTensor(),  transforms.Lambda(lambda x: torch.flatten(x))\n",
    "] )\n",
    "transforms_test = torchvision.transforms.Compose( [\n",
    "                                                    #transforms.Resize((28,28)),\n",
    "                                                    transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))\n",
    "] )\n",
    "\n",
    "#transforms_train = None\n",
    "#transforms_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SDcOBbGCM8z"
   },
   "source": [
    "Please do not modify the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXHitzrYqNhY",
    "outputId": "0dfae96d-305f-469d-c059-8eee15c43a44"
   },
   "outputs": [],
   "source": [
    "# ==========DO NOT REMOVE==========\n",
    "#-dataset\n",
    "dataset = MNIST('/files/', train=True, download=True,\n",
    "                      transform=transforms_train\n",
    "                )\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [50000, 10000], generator=torch.Generator().manual_seed(14))\n",
    "\n",
    "test_dataset = MNIST('/files/', train=False, download=True,\n",
    "                      transform=transforms_test\n",
    "                     )\n",
    "#-dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#-creating a dir for saving results\n",
    "name = 'vae_bern_mg'\n",
    "result_dir = images_dir + 'results/' + name + '/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmKDXMI0B231"
   },
   "source": [
    "In the next cell, please initialize the model. Please remember about commenting your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGXdL3gBrrGY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "b73aaBDxqSYb"
   },
   "outputs": [],
   "source": [
    "# BASIC HYPERPARAMETERS\n",
    "#D = 784   # input dimension # 28X28\n",
    "D = 784\n",
    "L = 2  # number of latents\n",
    "\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped\n",
    "\n",
    "# For gaussian mixture\n",
    "num_components = 4**2\n",
    "\n",
    "# model definition\n",
    "# FILL IN\n",
    "likelihood_type = 'bernoulli'\n",
    "\n",
    "if likelihood_type == 'categorical':\n",
    "    num_vals = 17\n",
    "elif likelihood_type == 'bernoulli':\n",
    "    num_vals = 1\n",
    "    \n",
    "# YOUR CODE COMES HERE:\n",
    "# FILL IN ANY OTHER HYPERPARAMS YOU WANT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5kdnqbiSDmq",
    "outputId": "1e92d8c0-e99d-442d-f887-ec35fdc8a4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim :  784\n",
      "nb neurones :  256\n",
      "nb latents:  2\n",
      "2 * L :  4\n",
      "num_vals > 1\n",
      "num_vals * D :  784\n",
      "VAE by CR.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=256, out_features=784, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prior): Prior()\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INIT YOUR VAE (PLEASE CALL IT model)\n",
    "# AN EXAMPLE: model = VAE(encoder, decoder, likelihood_type=likelihood_type, ...)\n",
    "\n",
    "print(\"input dim : \",D)\n",
    "print(\"nb neurones : \",M)\n",
    "print(\"nb latents: \",L)\n",
    "print(\"2 * L : \",2 * L)\n",
    "print(\"num_vals >\",num_vals)\n",
    "print(\"num_vals * D : \",num_vals * D)\n",
    "\n",
    "encoder_net = nn.Sequential(nn.Linear(D, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, 2 * L))\n",
    "\n",
    "encoder = Encoder(encoder_net=encoder_net)\n",
    "decoder_net = nn.Sequential(nn.Linear(L, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, num_vals * D))\n",
    "\n",
    "decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "\n",
    "#prior = torch.distributions.MultivariateNormal(torch.zeros(L), torch.eye(L))\n",
    "prior =  Prior(L=L, num_components=num_components)\n",
    "#model = VAE(encoder=encoder, decoder=decoder, num_vals=num_vals, prior=L,num_compo=num_components, likelihood_type=likelihood_type)\n",
    "model = VAE(encoder, decoder, prior, num_vals=num_vals, L=L, likelihood_type=likelihood_type)\n",
    "\n",
    "# Print the summary (like in Keras)\n",
    "#print(\"ENCODER:\\n\", summary(encoder, torch.zeros(1, D), show_input=True, show_hierarchical=True))\n",
    "#print(\"\\nDECODER:\\n\",  summary(decoder, torch.zeros(1, L), show_input=True, show_hierarchical=True))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC8AkWt4CURT"
   },
   "source": [
    "Please initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "a3nTSDe7ql08"
   },
   "outputs": [],
   "source": [
    "# PLEASE DEFINE YOUR OPTIMIZER\n",
    "lr = 1e-5 # learning rate (PLEASE CHANGE IT AS YOU WISH!)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79odxtRjCaix"
   },
   "source": [
    "#### Question 8\n",
    "\n",
    "Please explain the choice of the optimizer, and comment on the choice of the hyperparameters (e.g., the learing reate value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEjOlYN9Ft_B"
   },
   "source": [
    "**ANSWER:**\n",
    "I considered Adamax as in the Jacub example, that is not very adventurous but it led me to understand why this is a good optimizer\n",
    "\n",
    "Adamax is a extension desifn for accelerating Adam, a gradient descent algorithm that update the objective function after a given batch size.\n",
    "\n",
    "This one is able to deal with non stationnary objectives and sparse gradients as explained in the original paper : https://arxiv.org/pdf/1412.6980.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5GrzUcHFweG"
   },
   "source": [
    "### Training and final evaluation\n",
    "\n",
    "In the following two cells, we run the training and the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "VD7WuY6bqnBK",
    "outputId": "5ca36a19-11a8-416c-c698-0427fa34f137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>> epoch  0 / 1000\n",
      "Epoch: 0, val nll=314.96735698242185\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  1 / 1000\n",
      "Epoch: 1, val nll=275.5360330810547\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  2 / 1000\n",
      "Epoch: 2, val nll=265.0988737548828\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  3 / 1000\n",
      "Epoch: 3, val nll=260.1923396484375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  4 / 1000\n",
      "Epoch: 4, val nll=257.27139794921874\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  5 / 1000\n",
      "Epoch: 5, val nll=254.76557641601562\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  6 / 1000\n",
      "Epoch: 6, val nll=252.35672780761718\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  7 / 1000\n",
      "Epoch: 7, val nll=250.24391318359375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  8 / 1000\n",
      "Epoch: 8, val nll=248.5674984375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  9 / 1000\n",
      "Epoch: 9, val nll=247.3224782470703\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  10 / 1000\n",
      "Epoch: 10, val nll=246.27089418945312\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  11 / 1000\n",
      "Epoch: 11, val nll=245.45234008789063\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  12 / 1000\n",
      "Epoch: 12, val nll=244.86294567871093\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  13 / 1000\n",
      "Epoch: 13, val nll=244.33176076660158\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  14 / 1000\n",
      "Epoch: 14, val nll=243.89651015625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  15 / 1000\n",
      "Epoch: 15, val nll=243.50930720214845\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  16 / 1000\n",
      "Epoch: 16, val nll=243.20655380859375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  17 / 1000\n",
      "Epoch: 17, val nll=242.89246306152344\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  18 / 1000\n",
      "Epoch: 18, val nll=242.49788937988282\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  19 / 1000\n",
      "Epoch: 19, val nll=241.91160690917968\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  20 / 1000\n",
      "Epoch: 20, val nll=240.7369048828125\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  21 / 1000\n",
      "Epoch: 21, val nll=238.56248210449218\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  22 / 1000\n",
      "Epoch: 22, val nll=236.26766958007812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  23 / 1000\n",
      "Epoch: 23, val nll=234.61005424804688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  24 / 1000\n",
      "Epoch: 24, val nll=233.51198129882812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  25 / 1000\n",
      "Epoch: 25, val nll=232.77898291015626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  26 / 1000\n",
      "Epoch: 26, val nll=232.17084223632813\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  27 / 1000\n",
      "Epoch: 27, val nll=231.64061586914062\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  28 / 1000\n",
      "Epoch: 28, val nll=231.1600207519531\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  29 / 1000\n",
      "Epoch: 29, val nll=230.6548548339844\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  30 / 1000\n",
      "Epoch: 30, val nll=230.18648989257812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  31 / 1000\n",
      "Epoch: 31, val nll=229.71170483398438\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  32 / 1000\n",
      "Epoch: 32, val nll=229.26684270019533\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  33 / 1000\n",
      "Epoch: 33, val nll=228.79282595214843\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  34 / 1000\n",
      "Epoch: 34, val nll=228.40692778320312\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  35 / 1000\n",
      "Epoch: 35, val nll=227.9820302734375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  36 / 1000\n",
      "Epoch: 36, val nll=227.59380903320312\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  37 / 1000\n",
      "Epoch: 37, val nll=227.2457511230469\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  38 / 1000\n",
      "Epoch: 38, val nll=226.86940190429686\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  39 / 1000\n",
      "Epoch: 39, val nll=226.56681682128905\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  40 / 1000\n",
      "Epoch: 40, val nll=226.23965478515626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  41 / 1000\n",
      "Epoch: 41, val nll=225.98113420410155\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  42 / 1000\n",
      "Epoch: 42, val nll=225.71558471679688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  43 / 1000\n",
      "Epoch: 43, val nll=225.47843518066406\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  44 / 1000\n",
      "Epoch: 44, val nll=225.2226760986328\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  45 / 1000\n",
      "Epoch: 45, val nll=224.98725563964842\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  46 / 1000\n",
      "Epoch: 46, val nll=224.76859379882814\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  47 / 1000\n",
      "Epoch: 47, val nll=224.5892671875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  48 / 1000\n",
      "Epoch: 48, val nll=224.37378852539064\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  49 / 1000\n",
      "Epoch: 49, val nll=224.14087099609375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  50 / 1000\n",
      "Epoch: 50, val nll=224.0132669189453\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  51 / 1000\n",
      "Epoch: 51, val nll=223.8424346435547\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  52 / 1000\n",
      "Epoch: 52, val nll=223.66850803222655\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  53 / 1000\n",
      "Epoch: 53, val nll=223.4820792480469\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  54 / 1000\n",
      "Epoch: 54, val nll=223.33602197265625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  55 / 1000\n",
      "Epoch: 55, val nll=223.18180998535155\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  56 / 1000\n",
      "Epoch: 56, val nll=223.04934025878907\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  57 / 1000\n",
      "Epoch: 57, val nll=222.86163696289063\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  58 / 1000\n",
      "Epoch: 58, val nll=222.76373393554687\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  59 / 1000\n",
      "Epoch: 59, val nll=222.63175710449218\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  60 / 1000\n",
      "Epoch: 60, val nll=222.49002622070313\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  61 / 1000\n",
      "Epoch: 61, val nll=222.34361123046875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  62 / 1000\n",
      "Epoch: 62, val nll=222.20826623535157\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  63 / 1000\n",
      "Epoch: 63, val nll=222.10983818359375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  64 / 1000\n",
      "Epoch: 64, val nll=221.9751564453125\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  65 / 1000\n",
      "Epoch: 65, val nll=221.8499226074219\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  66 / 1000\n",
      "Epoch: 66, val nll=221.7063095703125\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  67 / 1000\n",
      "Epoch: 67, val nll=221.63331701660155\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  68 / 1000\n",
      "Epoch: 68, val nll=221.48241784667968\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  69 / 1000\n",
      "Epoch: 69, val nll=221.367577734375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  70 / 1000\n",
      "Epoch: 70, val nll=221.26617143554688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  71 / 1000\n",
      "Epoch: 71, val nll=221.16847822265626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  72 / 1000\n",
      "Epoch: 72, val nll=221.05408967285157\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  73 / 1000\n",
      "Epoch: 73, val nll=220.96755959472657\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  74 / 1000\n",
      "Epoch: 74, val nll=220.85624086914063\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  75 / 1000\n",
      "Epoch: 75, val nll=220.76379038085938\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  76 / 1000\n",
      "Epoch: 76, val nll=220.63873549804688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  77 / 1000\n",
      "Epoch: 77, val nll=220.53749934082032\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  78 / 1000\n",
      "Epoch: 78, val nll=220.46520339355467\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  79 / 1000\n",
      "Epoch: 79, val nll=220.37872490234375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  80 / 1000\n",
      "Epoch: 80, val nll=220.27678024902343\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  81 / 1000\n",
      "Epoch: 81, val nll=220.16301401367187\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  82 / 1000\n",
      "Epoch: 82, val nll=220.06346896972656\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  83 / 1000\n",
      "Epoch: 83, val nll=219.99681796875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  84 / 1000\n",
      "Epoch: 84, val nll=219.92431708984375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  85 / 1000\n",
      "Epoch: 85, val nll=219.78821794433594\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  86 / 1000\n",
      "Epoch: 86, val nll=219.68818754882813\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  87 / 1000\n",
      "Epoch: 87, val nll=219.59401794433595\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  88 / 1000\n",
      "Epoch: 88, val nll=219.52507734375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  89 / 1000\n",
      "Epoch: 89, val nll=219.43561416015626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  90 / 1000\n",
      "Epoch: 90, val nll=219.33321469726562\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  91 / 1000\n",
      "Epoch: 91, val nll=219.2542685546875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  92 / 1000\n",
      "Epoch: 92, val nll=219.14551147460938\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  93 / 1000\n",
      "Epoch: 93, val nll=219.10527482910157\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  94 / 1000\n",
      "Epoch: 94, val nll=218.99652316894532\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  95 / 1000\n",
      "Epoch: 95, val nll=218.93764951171875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  96 / 1000\n",
      "Epoch: 96, val nll=218.8480109375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  97 / 1000\n",
      "Epoch: 97, val nll=218.73792456054687\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  98 / 1000\n",
      "Epoch: 98, val nll=218.66465913085938\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  99 / 1000\n",
      "Epoch: 99, val nll=218.60827290039063\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  100 / 1000\n",
      "Epoch: 100, val nll=218.5236921875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  101 / 1000\n",
      "Epoch: 101, val nll=218.43992556152344\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  102 / 1000\n",
      "Epoch: 102, val nll=218.3624541015625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  103 / 1000\n",
      "Epoch: 103, val nll=218.22387280273438\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  104 / 1000\n",
      "Epoch: 104, val nll=218.15223132324218\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  105 / 1000\n",
      "Epoch: 105, val nll=218.07669743652343\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  106 / 1000\n",
      "Epoch: 106, val nll=218.04293540039063\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  107 / 1000\n",
      "Epoch: 107, val nll=217.95735166015626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  108 / 1000\n",
      "Epoch: 108, val nll=217.86534213867188\n",
      "saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>> epoch  109 / 1000\n",
      "Epoch: 109, val nll=217.8172957763672\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  110 / 1000\n",
      "Epoch: 110, val nll=217.72037841796876\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  111 / 1000\n",
      "Epoch: 111, val nll=217.63186086425782\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  112 / 1000\n",
      "Epoch: 112, val nll=217.59454716796876\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  113 / 1000\n",
      "Epoch: 113, val nll=217.46168818359374\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  114 / 1000\n",
      "Epoch: 114, val nll=217.38583745117188\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  115 / 1000\n",
      "Epoch: 115, val nll=217.29655539550782\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  116 / 1000\n",
      "Epoch: 116, val nll=217.25532238769532\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  117 / 1000\n",
      "Epoch: 117, val nll=217.17374897460937\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  118 / 1000\n",
      "Epoch: 118, val nll=217.0603259765625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  119 / 1000\n",
      "Epoch: 119, val nll=217.0105115234375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  120 / 1000\n",
      "Epoch: 120, val nll=216.92694841308594\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  121 / 1000\n",
      "Epoch: 121, val nll=216.89547097167969\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  122 / 1000\n",
      "Epoch: 122, val nll=216.76930036621093\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  123 / 1000\n",
      "Epoch: 123, val nll=216.65294738769532\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  124 / 1000\n",
      "Epoch: 124, val nll=216.63250368652345\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  125 / 1000\n",
      "Epoch: 125, val nll=216.54983525390625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  126 / 1000\n",
      "Epoch: 126, val nll=216.49236059570313\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  127 / 1000\n",
      "Epoch: 127, val nll=216.39868337402345\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  128 / 1000\n",
      "Epoch: 128, val nll=216.31791696777344\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  129 / 1000\n",
      "Epoch: 129, val nll=216.26114580078124\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  130 / 1000\n",
      "Epoch: 130, val nll=216.17979450683595\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  131 / 1000\n",
      "Epoch: 131, val nll=216.11320727539064\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  132 / 1000\n",
      "Epoch: 132, val nll=216.0483516357422\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  133 / 1000\n",
      "Epoch: 133, val nll=215.99726862792969\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  134 / 1000\n",
      "Epoch: 134, val nll=215.89691635742187\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  135 / 1000\n",
      "Epoch: 135, val nll=215.81506762695312\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  136 / 1000\n",
      "Epoch: 136, val nll=215.77614228515625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  137 / 1000\n",
      "Epoch: 137, val nll=215.68424755859374\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  138 / 1000\n",
      "Epoch: 138, val nll=215.639679296875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  139 / 1000\n",
      "Epoch: 139, val nll=215.57680888671874\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  140 / 1000\n",
      "Epoch: 140, val nll=215.48889919433594\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  141 / 1000\n",
      "Epoch: 141, val nll=215.41105920410158\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  142 / 1000\n",
      "Epoch: 142, val nll=215.3258013671875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  143 / 1000\n",
      "Epoch: 143, val nll=215.32244921875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  144 / 1000\n",
      "Epoch: 144, val nll=215.20133601074218\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  145 / 1000\n",
      "Epoch: 145, val nll=215.12320454101564\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  146 / 1000\n",
      "Epoch: 146, val nll=215.0546971923828\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  147 / 1000\n",
      "Epoch: 147, val nll=214.99557888183594\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  148 / 1000\n",
      "Epoch: 148, val nll=214.92748955078125\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  149 / 1000\n",
      "Epoch: 149, val nll=214.90358286132812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  150 / 1000\n",
      "Epoch: 150, val nll=214.8172603515625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  151 / 1000\n",
      "Epoch: 151, val nll=214.76909807128905\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  152 / 1000\n",
      "Epoch: 152, val nll=214.7421932128906\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  153 / 1000\n",
      "Epoch: 153, val nll=214.6017717529297\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  154 / 1000\n",
      "Epoch: 154, val nll=214.56285786132813\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  155 / 1000\n",
      "Epoch: 155, val nll=214.54086943359374\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  156 / 1000\n",
      "Epoch: 156, val nll=214.46441904296876\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  157 / 1000\n",
      "Epoch: 157, val nll=214.3960081298828\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  158 / 1000\n",
      "Epoch: 158, val nll=214.34155920410157\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  159 / 1000\n",
      "Epoch: 159, val nll=214.29120251464843\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  160 / 1000\n",
      "Epoch: 160, val nll=214.23859431152343\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  161 / 1000\n",
      "Epoch: 161, val nll=214.16688217773438\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  162 / 1000\n",
      "Epoch: 162, val nll=214.14362124023438\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  163 / 1000\n",
      "Epoch: 163, val nll=214.0901727294922\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  164 / 1000\n",
      "Epoch: 164, val nll=214.02023461914064\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  165 / 1000\n",
      "Epoch: 165, val nll=213.94251169433593\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  166 / 1000\n",
      "Epoch: 166, val nll=213.87840756835936\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  167 / 1000\n",
      "Epoch: 167, val nll=213.81455942382811\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  168 / 1000\n",
      "Epoch: 168, val nll=213.78083662109375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  169 / 1000\n",
      "Epoch: 169, val nll=213.7086287109375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  170 / 1000\n",
      "Epoch: 170, val nll=213.67555698242188\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  171 / 1000\n",
      "Epoch: 171, val nll=213.61268247070313\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  172 / 1000\n",
      "Epoch: 172, val nll=213.556386328125\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  173 / 1000\n",
      "Epoch: 173, val nll=213.52500478515626\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  174 / 1000\n",
      "Epoch: 174, val nll=213.48501252441406\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  175 / 1000\n",
      "Epoch: 175, val nll=213.41506831054687\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  176 / 1000\n",
      "Epoch: 176, val nll=213.38730048828126\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  177 / 1000\n",
      "Epoch: 177, val nll=213.30291083984375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  178 / 1000\n",
      "Epoch: 178, val nll=213.30849226074218\n",
      ">>>>>>>>>>>>> epoch  179 / 1000\n",
      "Epoch: 179, val nll=213.19470551757811\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  180 / 1000\n",
      "Epoch: 180, val nll=213.19560935058593\n",
      ">>>>>>>>>>>>> epoch  181 / 1000\n",
      "Epoch: 181, val nll=213.1138296142578\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  182 / 1000\n",
      "Epoch: 182, val nll=213.05816391601562\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  183 / 1000\n",
      "Epoch: 183, val nll=213.0558744873047\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  184 / 1000\n",
      "Epoch: 184, val nll=212.94540505371094\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  185 / 1000\n",
      "Epoch: 185, val nll=212.92406181640624\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  186 / 1000\n",
      "Epoch: 186, val nll=212.8508469970703\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  187 / 1000\n",
      "Epoch: 187, val nll=212.8566485107422\n",
      ">>>>>>>>>>>>> epoch  188 / 1000\n",
      "Epoch: 188, val nll=212.7399494873047\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  189 / 1000\n",
      "Epoch: 189, val nll=212.7387522705078\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  190 / 1000\n",
      "Epoch: 190, val nll=212.68093312988282\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  191 / 1000\n",
      "Epoch: 191, val nll=212.6753536621094\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  192 / 1000\n",
      "Epoch: 192, val nll=212.58757827148438\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  193 / 1000\n",
      "Epoch: 193, val nll=212.5808384765625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  194 / 1000\n",
      "Epoch: 194, val nll=212.47458408203124\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  195 / 1000\n",
      "Epoch: 195, val nll=212.45318239746095\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  196 / 1000\n",
      "Epoch: 196, val nll=212.44251301269531\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  197 / 1000\n",
      "Epoch: 197, val nll=212.3803681640625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  198 / 1000\n",
      "Epoch: 198, val nll=212.35508374023436\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  199 / 1000\n",
      "Epoch: 199, val nll=212.30397573242186\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  200 / 1000\n",
      "Epoch: 200, val nll=212.27560275878906\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  201 / 1000\n",
      "Epoch: 201, val nll=212.21588386230468\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  202 / 1000\n",
      "Epoch: 202, val nll=212.15341411132812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  203 / 1000\n",
      "Epoch: 203, val nll=212.14065029296876\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  204 / 1000\n",
      "Epoch: 204, val nll=212.093732421875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  205 / 1000\n",
      "Epoch: 205, val nll=212.05071862792968\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  206 / 1000\n",
      "Epoch: 206, val nll=212.04321364746093\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  207 / 1000\n",
      "Epoch: 207, val nll=211.96794228515625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  208 / 1000\n",
      "Epoch: 208, val nll=211.93617678222657\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  209 / 1000\n",
      "Epoch: 209, val nll=211.87918793945312\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  210 / 1000\n",
      "Epoch: 210, val nll=211.8606414794922\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  211 / 1000\n",
      "Epoch: 211, val nll=211.82528728027344\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  212 / 1000\n",
      "Epoch: 212, val nll=211.75605458984376\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  213 / 1000\n",
      "Epoch: 213, val nll=211.73935192871093\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  214 / 1000\n",
      "Epoch: 214, val nll=211.63626213378907\n",
      "saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>> epoch  215 / 1000\n",
      "Epoch: 215, val nll=211.65697661132813\n",
      ">>>>>>>>>>>>> epoch  216 / 1000\n",
      "Epoch: 216, val nll=211.625004296875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  217 / 1000\n",
      "Epoch: 217, val nll=211.56822380371094\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  218 / 1000\n",
      "Epoch: 218, val nll=211.57252846679688\n",
      ">>>>>>>>>>>>> epoch  219 / 1000\n",
      "Epoch: 219, val nll=211.53095236816407\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  220 / 1000\n",
      "Epoch: 220, val nll=211.45523315429688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  221 / 1000\n",
      "Epoch: 221, val nll=211.4794091796875\n",
      ">>>>>>>>>>>>> epoch  222 / 1000\n",
      "Epoch: 222, val nll=211.36594926757812\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  223 / 1000\n",
      "Epoch: 223, val nll=211.3451244140625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  224 / 1000\n",
      "Epoch: 224, val nll=211.33345251464843\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  225 / 1000\n",
      "Epoch: 225, val nll=211.30485505371092\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  226 / 1000\n",
      "Epoch: 226, val nll=211.22915139160156\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  227 / 1000\n",
      "Epoch: 227, val nll=211.23013896484375\n",
      ">>>>>>>>>>>>> epoch  228 / 1000\n",
      "Epoch: 228, val nll=211.20158315429688\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  229 / 1000\n",
      "Epoch: 229, val nll=211.12588989257813\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  230 / 1000\n",
      "Epoch: 230, val nll=211.1221221435547\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  231 / 1000\n",
      "Epoch: 231, val nll=211.07364265136718\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  232 / 1000\n",
      "Epoch: 232, val nll=211.05164516601562\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  233 / 1000\n",
      "Epoch: 233, val nll=211.03529638671876\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  234 / 1000\n",
      "Epoch: 234, val nll=210.96907299804687\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  235 / 1000\n",
      "Epoch: 235, val nll=210.97287456054687\n",
      ">>>>>>>>>>>>> epoch  236 / 1000\n",
      "Epoch: 236, val nll=210.91113195800781\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  237 / 1000\n",
      "Epoch: 237, val nll=210.83571728515625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  238 / 1000\n",
      "Epoch: 238, val nll=210.78892729492188\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  239 / 1000\n",
      "Epoch: 239, val nll=210.79482004394532\n",
      ">>>>>>>>>>>>> epoch  240 / 1000\n",
      "Epoch: 240, val nll=210.80390480957033\n",
      ">>>>>>>>>>>>> epoch  241 / 1000\n",
      "Epoch: 241, val nll=210.75453513183595\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  242 / 1000\n",
      "Epoch: 242, val nll=210.7097974609375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  243 / 1000\n",
      "Epoch: 243, val nll=210.6827380859375\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  244 / 1000\n",
      "Epoch: 244, val nll=210.6489181640625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  245 / 1000\n",
      "Epoch: 245, val nll=210.64955295410155\n",
      ">>>>>>>>>>>>> epoch  246 / 1000\n",
      "Epoch: 246, val nll=210.53887680664062\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  247 / 1000\n",
      "Epoch: 247, val nll=210.5903728515625\n",
      ">>>>>>>>>>>>> epoch  248 / 1000\n",
      "Epoch: 248, val nll=210.55605849609375\n",
      ">>>>>>>>>>>>> epoch  249 / 1000\n",
      "Epoch: 249, val nll=210.52581801757813\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  250 / 1000\n",
      "Epoch: 250, val nll=210.49868942871095\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  251 / 1000\n",
      "Epoch: 251, val nll=210.44443505859374\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  252 / 1000\n",
      "Epoch: 252, val nll=210.3846448486328\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  253 / 1000\n",
      "Epoch: 253, val nll=210.3803005126953\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  254 / 1000\n",
      "Epoch: 254, val nll=210.34070791015625\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  255 / 1000\n",
      "Epoch: 255, val nll=210.36608356933593\n",
      ">>>>>>>>>>>>> epoch  256 / 1000\n",
      "Epoch: 256, val nll=210.3010101074219\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  257 / 1000\n",
      "Epoch: 257, val nll=210.2364163330078\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  258 / 1000\n",
      "Epoch: 258, val nll=210.25583369140625\n",
      ">>>>>>>>>>>>> epoch  259 / 1000\n",
      "Epoch: 259, val nll=210.19352612304687\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  260 / 1000\n",
      "Epoch: 260, val nll=210.11971267089845\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  261 / 1000\n",
      "Epoch: 261, val nll=210.18566643066407\n",
      ">>>>>>>>>>>>> epoch  262 / 1000\n",
      "Epoch: 262, val nll=210.14533369140625\n",
      ">>>>>>>>>>>>> epoch  263 / 1000\n",
      "Epoch: 263, val nll=210.11326604003906\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  264 / 1000\n",
      "Epoch: 264, val nll=210.03026943359376\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  265 / 1000\n",
      "Epoch: 265, val nll=210.04126101074218\n",
      ">>>>>>>>>>>>> epoch  266 / 1000\n",
      "Epoch: 266, val nll=209.970057421875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  267 / 1000\n",
      "Epoch: 267, val nll=209.9578059082031\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  268 / 1000\n",
      "Epoch: 268, val nll=209.91098312988282\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  269 / 1000\n",
      "Epoch: 269, val nll=209.92792990722657\n",
      ">>>>>>>>>>>>> epoch  270 / 1000\n",
      "Epoch: 270, val nll=209.9101786621094\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  271 / 1000\n",
      "Epoch: 271, val nll=209.82960783691405\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  272 / 1000\n",
      "Epoch: 272, val nll=209.86806711425783\n",
      ">>>>>>>>>>>>> epoch  273 / 1000\n",
      "Epoch: 273, val nll=209.77784802246094\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  274 / 1000\n",
      "Epoch: 274, val nll=209.779812109375\n",
      ">>>>>>>>>>>>> epoch  275 / 1000\n",
      "Epoch: 275, val nll=209.79530939941407\n",
      ">>>>>>>>>>>>> epoch  276 / 1000\n",
      "Epoch: 276, val nll=209.72864755859376\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  277 / 1000\n",
      "Epoch: 277, val nll=209.6853335205078\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  278 / 1000\n",
      "Epoch: 278, val nll=209.70325649414062\n",
      ">>>>>>>>>>>>> epoch  279 / 1000\n",
      "Epoch: 279, val nll=209.65054321289062\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  280 / 1000\n",
      "Epoch: 280, val nll=209.6957353515625\n",
      ">>>>>>>>>>>>> epoch  281 / 1000\n",
      "Epoch: 281, val nll=209.62303591308594\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  282 / 1000\n",
      "Epoch: 282, val nll=209.58937702636717\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  283 / 1000\n",
      "Epoch: 283, val nll=209.53391557617186\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  284 / 1000\n",
      "Epoch: 284, val nll=209.57356689453124\n",
      ">>>>>>>>>>>>> epoch  285 / 1000\n",
      "Epoch: 285, val nll=209.49217392578126\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  286 / 1000\n",
      "Epoch: 286, val nll=209.51011877441405\n",
      ">>>>>>>>>>>>> epoch  287 / 1000\n",
      "Epoch: 287, val nll=209.45437788085937\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  288 / 1000\n",
      "Epoch: 288, val nll=209.44419418945313\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  289 / 1000\n",
      "Epoch: 289, val nll=209.3993546875\n",
      "saved!\n",
      ">>>>>>>>>>>>> epoch  290 / 1000\n",
      "Epoch: 290, val nll=209.41144638671875\n",
      ">>>>>>>>>>>>> epoch  291 / 1000\n"
     ]
    }
   ],
   "source": [
    "# ==========DO NOT REMOVE OR MODIFY==========\n",
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience,\n",
    "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                   training_loader=train_loader, val_loader=val_loader,\n",
    "                   shape=(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAuMt9_wquOI"
   },
   "outputs": [],
   "source": [
    "# ==========DO NOT REMOVE OR MODIFY==========\n",
    "# Final evaluation\n",
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "### JUST ADDED FINAL FOR BETTER COMPARISON\n",
    "samples_real(result_dir + name+'_FINAL', test_loader)\n",
    "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaBwGtSJF8ag"
   },
   "source": [
    "### Results and discussion\n",
    "\n",
    "After a successful training of your model, we would like to ask you to present your data and analyze it. Please answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ko6y4sp3fNAP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4WZkoiHFyZm"
   },
   "source": [
    "#### Question 9 TODO\n",
    "\n",
    "Please select the real data, and the final generated data and include them in this report. Please comment on the following:\n",
    "- Do you think the model was trained properly by looking at the generations? Please motivate your answer well.\n",
    "- What are the potential problems with evaluating a generative model by looking at generated data? How can we evaluate generative models (NOTE: ELBO or NLL do not count as answers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8yQ2T9GIuvc"
   },
   "source": [
    "ANSWER: [Please fill in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmyH318fIwc9"
   },
   "source": [
    "#### Question 10\n",
    "\n",
    "Please include the plot of the negative ELBO. Please comment on the following:\n",
    "- Is the training of your VAE stable or unstable? Why?\n",
    "- What is the influence of the optimizer on your model? Do the hyperparameter values of the optimizer important and how do they influence the training? Motivate well your answer (e.g., run the script with more than one learning rate and present two plots here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-10GAVZtKTj2"
   },
   "source": [
    "ANSWER: [Please fill in]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
